{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for image classification\n",
    "\n",
    "## Genesis Quiles-Galarza\n",
    "\n",
    "We will train convolutional neural networks to solve a binary classification task on a realistic case involving images obtained from image flow cytometry of red blood cells (available for download in the Github repository). The total number of provided images is 3469 and the original image resolution is $200Ã—200$. We will use 80% of the data to train your model and 20% to validate your predictions. We will experiment with different convolutional architectures, pre-processing or data augmentation techniques (e.g. image downsampling, translations, rotations, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "import pylab as p\n",
    "import scipy\n",
    "from scipy import integrate\n",
    "from pyDOE import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is originally sourced and has been modified from Paris Perdikaris's\n",
    "# Data Loader code posted on Github and found at the following link:\n",
    "# https://github.com/PredictiveIntelligenceLab/ENM531/blob/master/Week7/dataloader.py\n",
    "\n",
    "# Data Loader\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class Cell_dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Pocked Cell dataset.\"\"\"\n",
    "    def __init__(self, file_location):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.location = file_location\n",
    "        self.filenames = os.listdir(self.location)\n",
    "        # self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        name = self.filenames[idx]\n",
    "        #img = cv2.imread(self.location+name, cv2.IMREAD_GRAYSCALE)\n",
    "        #img = np.array(img)\n",
    "        #img = img.astype('float')\n",
    "        #img = np.reshape(img,(1,200,200))\n",
    "        nm = name.split(\"_\")\n",
    "        if nm[0]==\"Pocked\":\n",
    "            label = 1\n",
    "        elif nm[0]==\"Unpocked\":\n",
    "            label = 0\n",
    "        else:\n",
    "            print(\"String Parsing Error\")\n",
    "        \n",
    "        label = np.array(label).astype('float')\n",
    "        label = np.reshape(label,(1))\n",
    "        #Apply transformations here\n",
    "        img = Image.open(self.location + name)\n",
    "        img = transforms.functional.to_grayscale(img)\n",
    "        img = transforms.functional.resize(img, size=(20, 20))\n",
    "        h_flip = transforms.RandomHorizontalFlip(0.25)\n",
    "        v_flip = transforms.RandomVerticalFlip(0.25)\n",
    "        rotate = transforms.RandomRotation((-180, 180))\n",
    "        img = h_flip(img)\n",
    "        img = v_flip(img)\n",
    "        img = rotate(img)\n",
    "        img_to_tensor = transforms.ToTensor()\n",
    "        img = img_to_tensor(img)\n",
    "        return img, torch.from_numpy(label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is originally sourced and has been modified from Paris Perdikaris's\n",
    "# CNN code posted on Github and found at the following link:\n",
    "# https://github.com/PredictiveIntelligenceLab/ENM531/blob/master/Week7/models_torch.py\n",
    "\n",
    "# Define CNN architecture and forward pass\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.norm1 = torch.nn.BatchNorm2d(16)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.norm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.norm3 = torch.nn.BatchNorm2d(64)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = torch.nn.Linear(64 * 2 * 2, 2)\n",
    "        self.dropout = torch.nn.Dropout2d(0.1)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        out = self.ReLU(self.norm1(self.conv1(x)))\n",
    "        out = self.pool(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.ReLU(self.norm2(self.conv2(out)))\n",
    "        out = self.pool(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.ReLU(self.norm3(self.conv3(out)))\n",
    "        out = self.pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvNet:\n",
    "    # Initialize the class\n",
    "    def __init__(self, train_data, test_data):  \n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.net = CNN()\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "        \n",
    "           \n",
    "    # Trains the model by minimizing the Cross Entropy loss\n",
    "    def train(self, num_epochs = 10, batch_size = 128):\n",
    "        # Create a PyTorch data loader object\n",
    "        self.trainloader = torch.utils.data.DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(num_epochs):\n",
    "            for it, (images, labels) in enumerate(self.trainloader):\n",
    "                images = torch.autograd.Variable(images)\n",
    "                labels = torch.autograd.Variable(labels)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.net.forward_pass(images)\n",
    "                labels = labels.view(labels.shape[0])\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if it % 10 == 0:\n",
    "                    print (\"Epoch: {} | Iter: {} | Loss: {}\".format(epoch+1, it, loss.data)) \n",
    "                   \n",
    "                    \n",
    "    def test(self, batch_size = 128):\n",
    "        test_loader = torch.utils.data.DataLoader(self.test_data, batch_size=batch_size, shuffle=True)\n",
    "        # Test prediction accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = torch.autograd.Variable(images)\n",
    "            outputs = self.net.forward_pass(images)\n",
    "            _, predicted = torch.max(outputs.cpu().data, 1)\n",
    "            total += labels.shape[0]\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(\"Test accuracy on {} images: {:.4f}%\".format(len(self.test_data), correct / total))\n",
    "        \n",
    "    \n",
    "    # Evaluates predictions at test points    \n",
    "    #def predict(self, X_star):\n",
    "    #    X_star = torch.from_numpy(X_star).type(self.dtype_double) \n",
    "    #    X_star = Variable(X_star, requires_grad=False)\n",
    "    #    y_star = self.net.forward_pass(X_star)\n",
    "    #    y_star = y_star.cpu().data.numpy()\n",
    "    #    return y_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run applies no transformations to the images with number of epochs = 10 and batch size = 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 0 | Loss: 0.8126410245895386\n",
      "Epoch: 1 | Iter: 10 | Loss: 0.3877248167991638\n",
      "Epoch: 1 | Iter: 20 | Loss: 0.47052258253097534\n",
      "Epoch: 2 | Iter: 0 | Loss: 0.3528001606464386\n",
      "Epoch: 2 | Iter: 10 | Loss: 0.2877894937992096\n",
      "Epoch: 2 | Iter: 20 | Loss: 0.27889057993888855\n",
      "Epoch: 3 | Iter: 0 | Loss: 0.3289453983306885\n",
      "Epoch: 3 | Iter: 10 | Loss: 0.33050575852394104\n",
      "Epoch: 3 | Iter: 20 | Loss: 0.34997984766960144\n",
      "Epoch: 4 | Iter: 0 | Loss: 0.3750849664211273\n",
      "Epoch: 4 | Iter: 10 | Loss: 0.32393625378608704\n",
      "Epoch: 4 | Iter: 20 | Loss: 0.3052484691143036\n",
      "Epoch: 5 | Iter: 0 | Loss: 0.2982499599456787\n",
      "Epoch: 5 | Iter: 10 | Loss: 0.3614969849586487\n",
      "Epoch: 5 | Iter: 20 | Loss: 0.33477067947387695\n",
      "Epoch: 6 | Iter: 0 | Loss: 0.3695635497570038\n",
      "Epoch: 6 | Iter: 10 | Loss: 0.2905466854572296\n",
      "Epoch: 6 | Iter: 20 | Loss: 0.2598809003829956\n",
      "Epoch: 7 | Iter: 0 | Loss: 0.29965856671333313\n",
      "Epoch: 7 | Iter: 10 | Loss: 0.34567078948020935\n",
      "Epoch: 7 | Iter: 20 | Loss: 0.32998472452163696\n",
      "Epoch: 8 | Iter: 0 | Loss: 0.29502078890800476\n",
      "Epoch: 8 | Iter: 10 | Loss: 0.23854124546051025\n",
      "Epoch: 8 | Iter: 20 | Loss: 0.2950948476791382\n",
      "Epoch: 9 | Iter: 0 | Loss: 0.234140545129776\n",
      "Epoch: 9 | Iter: 10 | Loss: 0.3165559470653534\n",
      "Epoch: 9 | Iter: 20 | Loss: 0.263021320104599\n",
      "Epoch: 10 | Iter: 0 | Loss: 0.26726436614990234\n",
      "Epoch: 10 | Iter: 10 | Loss: 0.2744978368282318\n",
      "Epoch: 10 | Iter: 20 | Loss: 0.26600202918052673\n",
      "Test accuracy on 694 images: 60.7205%\n"
     ]
    }
   ],
   "source": [
    "test_data = Cell_dataset('test/')\n",
    "train_data = Cell_dataset('train/')\n",
    "\n",
    "cnn = ConvNet(train_data, test_data)\n",
    "cnn.train()\n",
    "cnn.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run applies no random transformations to the images with number of epochs = 10 and batch size = 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 0 | Loss: 0.8099663853645325\n",
      "Epoch: 1 | Iter: 10 | Loss: 0.4683122932910919\n",
      "Epoch: 1 | Iter: 20 | Loss: 0.3403487801551819\n",
      "Epoch: 1 | Iter: 30 | Loss: 0.359496533870697\n",
      "Epoch: 1 | Iter: 40 | Loss: 0.4109576344490051\n",
      "Epoch: 2 | Iter: 0 | Loss: 0.2967786490917206\n",
      "Epoch: 2 | Iter: 10 | Loss: 0.2813061773777008\n",
      "Epoch: 2 | Iter: 20 | Loss: 0.4074293076992035\n",
      "Epoch: 2 | Iter: 30 | Loss: 0.4051297903060913\n",
      "Epoch: 2 | Iter: 40 | Loss: 0.34555450081825256\n",
      "Epoch: 3 | Iter: 0 | Loss: 0.35366693139076233\n",
      "Epoch: 3 | Iter: 10 | Loss: 0.3505580425262451\n",
      "Epoch: 3 | Iter: 20 | Loss: 0.2707765996456146\n",
      "Epoch: 3 | Iter: 30 | Loss: 0.23460683226585388\n",
      "Epoch: 3 | Iter: 40 | Loss: 0.37340474128723145\n",
      "Epoch: 4 | Iter: 0 | Loss: 0.3771206736564636\n",
      "Epoch: 4 | Iter: 10 | Loss: 0.20156748592853546\n",
      "Epoch: 4 | Iter: 20 | Loss: 0.37737375497817993\n",
      "Epoch: 4 | Iter: 30 | Loss: 0.36035579442977905\n",
      "Epoch: 4 | Iter: 40 | Loss: 0.33306723833084106\n",
      "Epoch: 5 | Iter: 0 | Loss: 0.43771010637283325\n",
      "Epoch: 5 | Iter: 10 | Loss: 0.3298313617706299\n",
      "Epoch: 5 | Iter: 20 | Loss: 0.22423456609249115\n",
      "Epoch: 5 | Iter: 30 | Loss: 0.25235724449157715\n",
      "Epoch: 5 | Iter: 40 | Loss: 0.241842582821846\n",
      "Epoch: 6 | Iter: 0 | Loss: 0.3509856164455414\n",
      "Epoch: 6 | Iter: 10 | Loss: 0.24667617678642273\n",
      "Epoch: 6 | Iter: 20 | Loss: 0.25617745518684387\n",
      "Epoch: 6 | Iter: 30 | Loss: 0.3434601128101349\n",
      "Epoch: 6 | Iter: 40 | Loss: 0.39543408155441284\n",
      "Epoch: 7 | Iter: 0 | Loss: 0.40161773562431335\n",
      "Epoch: 7 | Iter: 10 | Loss: 0.2960509955883026\n",
      "Epoch: 7 | Iter: 20 | Loss: 0.16328221559524536\n",
      "Epoch: 7 | Iter: 30 | Loss: 0.39766013622283936\n",
      "Epoch: 7 | Iter: 40 | Loss: 0.2299182116985321\n",
      "Epoch: 8 | Iter: 0 | Loss: 0.24861054122447968\n",
      "Epoch: 8 | Iter: 10 | Loss: 0.23293213546276093\n",
      "Epoch: 8 | Iter: 20 | Loss: 0.22889822721481323\n",
      "Epoch: 8 | Iter: 30 | Loss: 0.34372225403785706\n",
      "Epoch: 8 | Iter: 40 | Loss: 0.3428369164466858\n",
      "Epoch: 9 | Iter: 0 | Loss: 0.2710835933685303\n",
      "Epoch: 9 | Iter: 10 | Loss: 0.22152230143547058\n",
      "Epoch: 9 | Iter: 20 | Loss: 0.13612158596515656\n",
      "Epoch: 9 | Iter: 30 | Loss: 0.3077801465988159\n",
      "Epoch: 9 | Iter: 40 | Loss: 0.28223931789398193\n",
      "Epoch: 10 | Iter: 0 | Loss: 0.13801348209381104\n",
      "Epoch: 10 | Iter: 10 | Loss: 0.22993579506874084\n",
      "Epoch: 10 | Iter: 20 | Loss: 0.375666081905365\n",
      "Epoch: 10 | Iter: 30 | Loss: 0.28448113799095154\n",
      "Epoch: 10 | Iter: 40 | Loss: 0.40463292598724365\n",
      "Test accuracy on 694 images: 31.5879%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "batch_size = 64\n",
    "\n",
    "cnn = ConvNet(train_data, test_data)\n",
    "cnn.train(num_epochs, batch_size)\n",
    "cnn.test(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run applies no transformations but doubles the amount of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 0 | Loss: 0.9448046684265137\n",
      "Epoch: 1 | Iter: 10 | Loss: 0.2997686266899109\n",
      "Epoch: 1 | Iter: 20 | Loss: 0.40549200773239136\n",
      "Epoch: 2 | Iter: 0 | Loss: 0.34627529978752136\n",
      "Epoch: 2 | Iter: 10 | Loss: 0.3997986912727356\n",
      "Epoch: 2 | Iter: 20 | Loss: 0.32744526863098145\n",
      "Epoch: 3 | Iter: 0 | Loss: 0.2999429702758789\n",
      "Epoch: 3 | Iter: 10 | Loss: 0.3831724524497986\n",
      "Epoch: 3 | Iter: 20 | Loss: 0.33958277106285095\n",
      "Epoch: 4 | Iter: 0 | Loss: 0.3591929078102112\n",
      "Epoch: 4 | Iter: 10 | Loss: 0.3027552664279938\n",
      "Epoch: 4 | Iter: 20 | Loss: 0.255239874124527\n",
      "Epoch: 5 | Iter: 0 | Loss: 0.31683242321014404\n",
      "Epoch: 5 | Iter: 10 | Loss: 0.28196799755096436\n",
      "Epoch: 5 | Iter: 20 | Loss: 0.29159659147262573\n",
      "Epoch: 6 | Iter: 0 | Loss: 0.3352051377296448\n",
      "Epoch: 6 | Iter: 10 | Loss: 0.2568396031856537\n",
      "Epoch: 6 | Iter: 20 | Loss: 0.35424166917800903\n",
      "Epoch: 7 | Iter: 0 | Loss: 0.22989685833454132\n",
      "Epoch: 7 | Iter: 10 | Loss: 0.30857276916503906\n",
      "Epoch: 7 | Iter: 20 | Loss: 0.2895723283290863\n",
      "Epoch: 8 | Iter: 0 | Loss: 0.37439048290252686\n",
      "Epoch: 8 | Iter: 10 | Loss: 0.3084217607975006\n",
      "Epoch: 8 | Iter: 20 | Loss: 0.2552608549594879\n",
      "Epoch: 9 | Iter: 0 | Loss: 0.29018697142601013\n",
      "Epoch: 9 | Iter: 10 | Loss: 0.2578703463077545\n",
      "Epoch: 9 | Iter: 20 | Loss: 0.24083343148231506\n",
      "Epoch: 10 | Iter: 0 | Loss: 0.22168771922588348\n",
      "Epoch: 10 | Iter: 10 | Loss: 0.3297065198421478\n",
      "Epoch: 10 | Iter: 20 | Loss: 0.25068825483322144\n",
      "Epoch: 11 | Iter: 0 | Loss: 0.26858800649642944\n",
      "Epoch: 11 | Iter: 10 | Loss: 0.29711219668388367\n",
      "Epoch: 11 | Iter: 20 | Loss: 0.30043935775756836\n",
      "Epoch: 12 | Iter: 0 | Loss: 0.20184780657291412\n",
      "Epoch: 12 | Iter: 10 | Loss: 0.2182948738336563\n",
      "Epoch: 12 | Iter: 20 | Loss: 0.2816825807094574\n",
      "Epoch: 13 | Iter: 0 | Loss: 0.21160735189914703\n",
      "Epoch: 13 | Iter: 10 | Loss: 0.21465662121772766\n",
      "Epoch: 13 | Iter: 20 | Loss: 0.25194257497787476\n",
      "Epoch: 14 | Iter: 0 | Loss: 0.15985123813152313\n",
      "Epoch: 14 | Iter: 10 | Loss: 0.24201783537864685\n",
      "Epoch: 14 | Iter: 20 | Loss: 0.18775177001953125\n",
      "Epoch: 15 | Iter: 0 | Loss: 0.1952313482761383\n",
      "Epoch: 15 | Iter: 10 | Loss: 0.24070192873477936\n",
      "Epoch: 15 | Iter: 20 | Loss: 0.20637667179107666\n",
      "Epoch: 16 | Iter: 0 | Loss: 0.14147469401359558\n",
      "Epoch: 16 | Iter: 10 | Loss: 0.17761702835559845\n",
      "Epoch: 16 | Iter: 20 | Loss: 0.21922287344932556\n",
      "Epoch: 17 | Iter: 0 | Loss: 0.15440784394741058\n",
      "Epoch: 17 | Iter: 10 | Loss: 0.1112319752573967\n",
      "Epoch: 17 | Iter: 20 | Loss: 0.17996136844158173\n",
      "Epoch: 18 | Iter: 0 | Loss: 0.14378602802753448\n",
      "Epoch: 18 | Iter: 10 | Loss: 0.12109450995922089\n",
      "Epoch: 18 | Iter: 20 | Loss: 0.13537807762622833\n",
      "Epoch: 19 | Iter: 0 | Loss: 0.15404628217220306\n",
      "Epoch: 19 | Iter: 10 | Loss: 0.10305778682231903\n",
      "Epoch: 19 | Iter: 20 | Loss: 0.2121286243200302\n",
      "Epoch: 20 | Iter: 0 | Loss: 0.16456852853298187\n",
      "Epoch: 20 | Iter: 10 | Loss: 0.1208728477358818\n",
      "Epoch: 20 | Iter: 20 | Loss: 0.1349492371082306\n",
      "Test accuracy on 694 images: 61.1787%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20 \n",
    "batch_size = 128\n",
    "\n",
    "cnn = ConvNet(train_data, test_data)\n",
    "cnn.train(num_epochs, batch_size)\n",
    "cnn.test(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous run had the best accuracy, let's try applying some random transformations to the images and seeing how that affects the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 0 | Loss: 0.8385254144668579\n",
      "Epoch: 1 | Iter: 10 | Loss: 0.4421956539154053\n",
      "Epoch: 1 | Iter: 20 | Loss: 0.3397660255432129\n",
      "Epoch: 2 | Iter: 0 | Loss: 0.36229249835014343\n",
      "Epoch: 2 | Iter: 10 | Loss: 0.3292085826396942\n",
      "Epoch: 2 | Iter: 20 | Loss: 0.333326131105423\n",
      "Epoch: 3 | Iter: 0 | Loss: 0.2863457500934601\n",
      "Epoch: 3 | Iter: 10 | Loss: 0.3336540758609772\n",
      "Epoch: 3 | Iter: 20 | Loss: 0.2811071276664734\n",
      "Epoch: 4 | Iter: 0 | Loss: 0.30463820695877075\n",
      "Epoch: 4 | Iter: 10 | Loss: 0.26561760902404785\n",
      "Epoch: 4 | Iter: 20 | Loss: 0.33842235803604126\n",
      "Epoch: 5 | Iter: 0 | Loss: 0.3095731735229492\n",
      "Epoch: 5 | Iter: 10 | Loss: 0.3091684579849243\n",
      "Epoch: 5 | Iter: 20 | Loss: 0.32981017231941223\n",
      "Epoch: 6 | Iter: 0 | Loss: 0.32305464148521423\n",
      "Epoch: 6 | Iter: 10 | Loss: 0.377492219209671\n",
      "Epoch: 6 | Iter: 20 | Loss: 0.2213761955499649\n",
      "Epoch: 7 | Iter: 0 | Loss: 0.29459109902381897\n",
      "Epoch: 7 | Iter: 10 | Loss: 0.31197166442871094\n",
      "Epoch: 7 | Iter: 20 | Loss: 0.2975875735282898\n",
      "Epoch: 8 | Iter: 0 | Loss: 0.30970853567123413\n",
      "Epoch: 8 | Iter: 10 | Loss: 0.35169485211372375\n",
      "Epoch: 8 | Iter: 20 | Loss: 0.3334343433380127\n",
      "Epoch: 9 | Iter: 0 | Loss: 0.27387019991874695\n",
      "Epoch: 9 | Iter: 10 | Loss: 0.322153776884079\n",
      "Epoch: 9 | Iter: 20 | Loss: 0.2802067697048187\n",
      "Epoch: 10 | Iter: 0 | Loss: 0.22958439588546753\n",
      "Epoch: 10 | Iter: 10 | Loss: 0.2662663757801056\n",
      "Epoch: 10 | Iter: 20 | Loss: 0.20265187323093414\n",
      "Epoch: 11 | Iter: 0 | Loss: 0.23275062441825867\n",
      "Epoch: 11 | Iter: 10 | Loss: 0.2332846224308014\n",
      "Epoch: 11 | Iter: 20 | Loss: 0.24897843599319458\n",
      "Epoch: 12 | Iter: 0 | Loss: 0.27436086535453796\n",
      "Epoch: 12 | Iter: 10 | Loss: 0.2320389300584793\n",
      "Epoch: 12 | Iter: 20 | Loss: 0.21213370561599731\n",
      "Epoch: 13 | Iter: 0 | Loss: 0.17953170835971832\n",
      "Epoch: 13 | Iter: 10 | Loss: 0.2740958333015442\n",
      "Epoch: 13 | Iter: 20 | Loss: 0.21889568865299225\n",
      "Epoch: 14 | Iter: 0 | Loss: 0.17680707573890686\n",
      "Epoch: 14 | Iter: 10 | Loss: 0.15866583585739136\n",
      "Epoch: 14 | Iter: 20 | Loss: 0.194283589720726\n",
      "Epoch: 15 | Iter: 0 | Loss: 0.18145865201950073\n",
      "Epoch: 15 | Iter: 10 | Loss: 0.1618141084909439\n",
      "Epoch: 15 | Iter: 20 | Loss: 0.20124943554401398\n",
      "Epoch: 16 | Iter: 0 | Loss: 0.12057390064001083\n",
      "Epoch: 16 | Iter: 10 | Loss: 0.13557951152324677\n",
      "Epoch: 16 | Iter: 20 | Loss: 0.1582571268081665\n",
      "Epoch: 17 | Iter: 0 | Loss: 0.12621456384658813\n",
      "Epoch: 17 | Iter: 10 | Loss: 0.1503991037607193\n",
      "Epoch: 17 | Iter: 20 | Loss: 0.14726680517196655\n",
      "Epoch: 18 | Iter: 0 | Loss: 0.10811010003089905\n",
      "Epoch: 18 | Iter: 10 | Loss: 0.13738222420215607\n",
      "Epoch: 18 | Iter: 20 | Loss: 0.18918034434318542\n",
      "Epoch: 19 | Iter: 0 | Loss: 0.18178606033325195\n",
      "Epoch: 19 | Iter: 10 | Loss: 0.06636248528957367\n",
      "Epoch: 19 | Iter: 20 | Loss: 0.1251426488161087\n",
      "Epoch: 20 | Iter: 0 | Loss: 0.12745258212089539\n",
      "Epoch: 20 | Iter: 10 | Loss: 0.13707564771175385\n",
      "Epoch: 20 | Iter: 20 | Loss: 0.10891517251729965\n",
      "Test accuracy on 694 images: 61.2392%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20 \n",
    "batch_size = 128\n",
    "\n",
    "cnn = ConvNet(train_data, test_data)\n",
    "cnn.train(num_epochs, batch_size)\n",
    "cnn.test(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations did not affect the outcome that much - which makes sense when you look at the images. They are all very similar in appearance and there's not much difference between a rotated or a flipped image.\n",
    "\n",
    "Unfortunately, I couldn't get this function to have any higher accuracy than ~60%. Some other options for improving this accuracy are downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
